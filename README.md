# MiniRAG - 最小 RAG 示例

基于 Go + Ollama + Chroma 实现的最小 RAG (检索增强生成) 系统示例。

## 技术栈

- **Go** - 编程语言
- **Ollama** - 本地 LLM 和 Embedding 服务
  - Embedding 模型: `qwen3-embedding`
  - LLM 模型: `qwen3.5:cloud`
- **Chroma** - 向量数据库 (使用 `chromem-go` 客户端)

## RAG 流程

```
┌─────────────────────────────────────────────────────────────────────┐
│                         RAG 流程                                    │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  提问前准备 (离线):                                                 │
│  ┌─────────┐    ┌─────────────┐    ┌─────────┐    ┌───────────┐  │
│  │ 原始文档 │ -> │    分片     │ -> │ 向量化  │ -> │ 存入向量库 │  │
│  │         │    │ (按段落切分) │    │(embedding)│   │  (Chroma)  │  │
│  └─────────┘    └─────────────┘    └─────────┘    └───────────┘  │
│                                                                     │
│  提问后处理 (在线):                                                  │
│  ┌─────────┐    ┌─────────┐    ┌─────────┐    ┌───────────────┐ │
│  │ 用户提问 │ -> │ 向量化  │ -> │   召回   │ -> │     重排      │ │
│  │         │    │         │    │ (topN)  │    │  (cross-encoder)│ │
│  └─────────┘    └─────────┘    └─────────┘    └───────────────┘ │
│                                                    │               │
│                                                    v               │
│                                          ┌───────────────┐        │
│                                          │   LLM 生成    │        │
│                                          │   (Ollama)    │        │
│                                          └───────────────┘        │
└─────────────────────────────────────────────────────────────────────┘
```

## 运行步骤

### 1. 安装依赖

```bash
go mod tidy
```

### 2. 启动 Ollama 服务

确保本地已安装 [Ollama](https://github.com/ollama/ollama)，并启动服务：

```bash
# 启动 Ollama 服务 (默认端口 11434)
ollama serve

# 拉取 Embedding 模型 (在新终端)
ollama pull qwen3-embedding:latest

# 拉取 LLM 模型 (在新终端)
ollama pull qwen3.5:cloud
```

### 3. 准备知识库文档

将待检索的文档放入 `docs/` 目录，目前仅支持纯文本文件 (.txt)。

示例文档格式见: `docs/milvus.txt`

### 4. 运行 RAG

```bash
go run main.go
```

## 代码结构

```
minirag/
├── main.go              # 主程序入口
├── go.mod               # Go 模块依赖
├── rag/
│   ├── chunk.go         # 文档分片
│   ├── embedding.go     # 向量化 (使用 Ollama)
│   ├── chroma.go        # 向量数据库存储与检索
│   ├── generate.go     # LLM 生成回答
│   └── ollama.go        # Ollama 客户端初始化
└── docs/
    └── milvus.txt       # 示例知识库文档
```

## 配置说明

如需修改默认配置，可编辑以下文件：

- `rag/embedding.go` - 修改 embedding 模型
- `rag/generate.go` - 修改 LLM 模型和 prompt 模板

## 注意事项

- 首次运行需要下载 Ollama 模型，请确保网络畅通
- 向量数据库为内存数据库，重启程序后数据会丢失
- 支持的文档格式目前仅为纯文本
